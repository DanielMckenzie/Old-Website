{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture : Text pre-processing, non-negative matrix factorization and topic modeling.\n",
    "\n",
    "In this lecture we'll discuss\n",
    "- Text-preprocessing techniques such as stemming and Tf-Idf.\n",
    "- Non-negative matrix factorization.\n",
    "- Some techniques for searching a document data set for matches to a query document.\n",
    "- Extracting meaningful topics from a corpus of documents.\n",
    "\n",
    "Note that **semantic** means \"relating to meaning in language or logic.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.decomposition import NMF\n",
    "import nltk\n",
    "import re\n",
    "import scipy.optimize.nnls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "from nltk.tokenize import word_tokenize\n",
    "data= fetch_20newsgroups(remove=('headers', 'footers', 'quotes')).data\n",
    "print(data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the bag-of-words approach. This means that we turn each article into an unordered list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.']\n"
     ]
    }
   ],
   "source": [
    "# Turn this article into a list of words\n",
    "Test_Article = word_tokenize(data[0])\n",
    "print(Test_Article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "- These are commonly used words that have little semantic value.\n",
    "- We typically remove them before performing any text analysis.\n",
    "- The definition of a stop word is not entirely rigourous, but different languages have different freely downloadable lists of stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', '.', '2-door', 'sports', 'car', ',', 'looked', 'late', '60s/', 'early', '70s', '.', 'called', 'bricklin', '.', 'doors', 'really', 'small', '.', 'addition', ',', 'front', 'bumper', 'separate', 'rest', 'body', '.', 'know', '.', 'anyone', 'tellme', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'production', ',', 'car', 'made', ',', 'history', ',', 'whatever', 'info', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(stop_words)\n",
    "Test_Article = [w.lower() for w in Test_Article]\n",
    "#print(Test_Article)\n",
    "Trimmed_Article = [w for w in Test_Article if not w in stop_words] \n",
    "print(Trimmed_Article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', '2-door', 'sports', 'car', 'looked', 'late', '60s/', 'early', '70s', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please', 'e-mail']\n"
     ]
    }
   ],
   "source": [
    "Trimmed_Article = [w for w in Trimmed_Article if not w=='.']\n",
    "Trimmed_Article = [w for w in Trimmed_Article if not w== ',']\n",
    "Trimmed_Article = [w for w in Trimmed_Article if not w== \"n't\"]\n",
    "print(Trimmed_Article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "- In English, the same base word can serve different grammatical roles.\n",
    "- E.g. \"swim\" can be made into \"swimming\", \"swimmers\" and \"swims\"\n",
    "- All of these words are the same semantically, that is they relate to the same topic or have the same meaning.\n",
    "- Stemming is the process of removing the suffixes, or ends of words, in a principled manner.\n",
    "- E.g. \"swimming\", \"swimmers\" and \"swims\" should all be stemmed to \"swims\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swim'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the stemmer function\n",
    "porter = nltk.PorterStemmer()\n",
    "porter.stem('swimming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wonder', 'anyon', 'could', 'enlighten', 'car', 'saw', 'day', '2-door', 'sport', 'car', 'look', 'late', '60s/', 'earli', '70', 'call', 'bricklin', 'door', 'realli', 'small', 'addit', 'front', 'bumper', 'separ', 'rest', 'bodi', 'know', 'anyon', 'tellm', 'model', 'name', 'engin', 'spec', 'year', 'product', 'car', 'made', 'histori', 'whatev', 'info', 'funki', 'look', 'car', 'pleas', 'e-mail']\n"
     ]
    }
   ],
   "source": [
    "# Now lets stem the article\n",
    "Trimmed_Article  = [porter.stem(w) for w in Trimmed_Article]\n",
    "print(Trimmed_Article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    " - Numbers?\n",
    " - some stop words may have snuck through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the corpus\n",
    "- **Terminology:** a collection of documents is called a *corpus*.\n",
    "- After stemming and removing stop words from our articles, we would like to **vectorize**\n",
    "- **vectorize:** Turn document $d_{j}$ into vector $\\mathbf{a}_j$.\n",
    "- **Goal** convert a corpus to a matrix $A$ where columns correspond to articles and rows to key-words.\n",
    "- **Question** How to choose keywords?\n",
    "- **Question** How to fill in $A_{ij}$?\n",
    "\n",
    "![alt text](term_document.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing key-words\n",
    "- As we've already removed stop-words, the remaining words are likely to be informative.\n",
    "- Compute the *corpus frequency* of all remaining words.\n",
    "- Choose the $d$ most frequently occuring words as keywords.\n",
    "\n",
    "![alt text](Word_Frequency.png)\n",
    "\n",
    "### Term weighting\n",
    "\n",
    "- Call the keywords $w_1,\\ldots, w_{d}$. \n",
    "- Count up the number of times $w_i$ appears in $d_j$.\n",
    "- If $w_i$ appears in $d_j$ frequently, it likely is telling us something about $d_j$.\n",
    "- On the other hand, if $w_i$ appears in all $d_j$, it is likely telling us something about the *corpus*, and therefore is not useful in distinguishing articles.\n",
    "- **Solution:** tf-idf weighting\n",
    "    - $\\text{tf}(w_i,d_j) = \\# \\text{times $w_i$ appears in $d_j$}$ \n",
    "    - $\\text{df}(w_i) = \\# \\text{articles containing $w_i$ at least once}$.\n",
    "    - $n = \\# \\text{of documents}$ \n",
    "    - $\\displaystyle A_{ij} = \\text{tf}(w_i,d_j)\\log\\left(\\frac{n}{\\text{df}(w_i)}\\right)$\n",
    "- Typically **normalize** columns: Replace $\\mathbf{a}_j$ with $\\mathbf{a}_j/\\|\\mathbf{a}_j\\|_2$\n",
    "\n",
    ">**Important:** Note that $A\\in\\mathbb{R}^{d\\times n}$ is *non-negative* \n",
    "\n",
    ">**Important:** Note that *columns* of $A$ correspond to data points. This is the usual convention in topic modeling, but different to the convention for PCA.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Practice\n",
    "This is all handled by the following two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "vectorizer = TfidfVectorizer(max_features=2000, min_df=10, stop_words='english')\n",
    "# max features = max_number of keywords to consider.\n",
    "# min_df = potential keywords with document frequency less than 10 will be ignored. This means we may get fewer than\n",
    "# max_features keywords\n",
    "A = vectorizer.fit_transform(data)\n",
    "# Above line actually does the tf-idf conversion\n",
    "idx_to_word = np.array(vectorizer.get_feature_names())\n",
    "# Above line will get and store the keywords w_i\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Important:** Note that by convention scikit learn puts data points as *rows* of $A$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00\n",
      "000\n",
      "01\n",
      "02\n",
      "03\n",
      "04\n",
      "05\n",
      "06\n",
      "0d\n",
      "0t\n",
      "10\n",
      "100\n",
      "1000\n",
      "11\n",
      "12\n",
      "128\n",
      "13\n",
      "130\n",
      "14\n",
      "145\n",
      "15\n",
      "150\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1d9\n",
      "1st\n",
      "1t\n",
      "20\n",
      "200\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "250\n",
      "256\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "2nd\n",
      "2tm\n",
      "30\n",
      "300\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "34u\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "386\n",
      "39\n",
      "3d\n",
      "3l\n",
      "3rd\n",
      "3t\n",
      "40\n",
      "400\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "486\n",
      "49\n",
      "4t\n",
      "50\n",
      "500\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "5u\n",
      "60\n",
      "600\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "6ei\n",
      "6g\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "75u\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "7ey\n",
      "7u\n",
      "80\n",
      "800\n",
      "81\n",
      "82\n",
      "84\n",
      "85\n",
      "86\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "95\n",
      "99\n",
      "9f\n",
      "9v\n",
      "__\n",
      "a86\n",
      "ability\n",
      "able\n",
      "absolute\n",
      "absolutely\n",
      "abuse\n",
      "ac\n",
      "accept\n",
      "accepted\n",
      "access\n",
      "according\n",
      "account\n",
      "accurate\n",
      "act\n",
      "action\n",
      "actions\n",
      "active\n",
      "activities\n",
      "activity\n",
      "acts\n",
      "actual\n",
      "actually\n",
      "ad\n",
      "add\n",
      "added\n",
      "addition\n",
      "additional\n",
      "address\n",
      "addresses\n",
      "administration\n",
      "admit\n",
      "advance\n",
      "advantage\n",
      "advice\n",
      "afraid\n",
      "age\n",
      "agencies\n",
      "agency\n",
      "agents\n",
      "ago\n",
      "agree\n",
      "ah\n",
      "ahead\n",
      "aid\n",
      "aids\n",
      "air\n",
      "al\n",
      "algorithm\n",
      "alive\n",
      "allow\n",
      "allowed\n",
      "allows\n",
      "alt\n",
      "alternative\n",
      "amendment\n",
      "america\n",
      "american\n",
      "americans\n",
      "amiga\n",
      "analysis\n",
      "andrew\n",
      "angeles\n",
      "animals\n",
      "announced\n",
      "annual\n",
      "anonymous\n",
      "answer\n",
      "answers\n",
      "anti\n",
      "anybody\n",
      "apartment\n",
      "app\n",
      "apparently\n",
      "appear\n",
      "appears\n",
      "apple\n",
      "application\n",
      "applications\n",
      "applied\n",
      "apply\n",
      "appreciate\n",
      "appreciated\n",
      "approach\n",
      "appropriate\n",
      "apr\n",
      "april\n",
      "arab\n",
      "arabs\n",
      "archive\n",
      "area\n",
      "areas\n",
      "aren\n",
      "argue\n",
      "argument\n",
      "arguments\n",
      "arm\n",
      "armed\n",
      "armenia\n",
      "armenian\n",
      "armenians\n",
      "arms\n",
      "army\n",
      "art\n",
      "article\n",
      "articles\n",
      "aside\n",
      "ask\n",
      "asked\n",
      "asking\n",
      "assault\n",
      "associated\n",
      "association\n",
      "assume\n",
      "assuming\n",
      "atheism\n",
      "atheist\n",
      "atheists\n",
      "attack\n",
      "attacks\n",
      "attempt\n",
      "attention\n",
      "attitude\n",
      "au\n",
      "audio\n",
      "author\n",
      "authorities\n",
      "authority\n",
      "authors\n",
      "auto\n",
      "automatic\n",
      "available\n",
      "average\n",
      "avoid\n",
      "aware\n",
      "away\n",
      "ax\n",
      "azerbaijan\n",
      "azerbaijani\n",
      "b8\n",
      "b8e\n",
      "b8f\n",
      "background\n",
      "bad\n",
      "ball\n",
      "banks\n",
      "base\n",
      "baseball\n",
      "based\n",
      "basic\n",
      "basically\n",
      "basis\n",
      "batf\n",
      "battery\n",
      "bay\n",
      "bbs\n",
      "bear\n",
      "beat\n",
      "began\n",
      "begin\n",
      "beginning\n",
      "behavior\n",
      "belief\n",
      "beliefs\n",
      "believe\n",
      "believed\n",
      "benefit\n",
      "berkeley\n",
      "best\n",
      "better\n",
      "bh\n",
      "bhj\n",
      "bible\n",
      "big\n",
      "bike\n",
      "billion\n",
      "bios\n",
      "bit\n",
      "bits\n",
      "bj\n",
      "black\n",
      "block\n",
      "blood\n",
      "blue\n",
      "board\n",
      "bob\n",
      "bodies\n",
      "body\n",
      "book\n",
      "books\n",
      "born\n",
      "bos\n",
      "boston\n",
      "bought\n",
      "box\n",
      "brand\n",
      "break\n",
      "brian\n",
      "bring\n",
      "british\n",
      "broken\n",
      "brother\n",
      "brought\n",
      "brown\n",
      "btw\n",
      "budget\n",
      "buf\n",
      "build\n",
      "building\n",
      "built\n",
      "bunch\n",
      "bus\n",
      "bush\n",
      "business\n",
      "button\n",
      "buy\n",
      "buying\n",
      "bxn\n",
      "byte\n",
      "bytes\n",
      "c8\n",
      "ca\n",
      "cable\n",
      "cache\n",
      "cal\n",
      "calgary\n",
      "california\n",
      "called\n",
      "calling\n",
      "calls\n",
      "came\n",
      "canada\n",
      "canadian\n",
      "cancer\n",
      "capable\n",
      "car\n",
      "card\n",
      "cards\n",
      "care\n",
      "carried\n",
      "carry\n",
      "carrying\n",
      "cars\n",
      "case\n",
      "cases\n",
      "catholic\n",
      "caught\n",
      "cause\n",
      "caused\n",
      "causes\n",
      "cc\n",
      "cd\n",
      "center\n",
      "central\n",
      "century\n",
      "certain\n",
      "certainly\n",
      "chance\n",
      "change\n",
      "changed\n",
      "changes\n",
      "changing\n",
      "channel\n",
      "char\n",
      "character\n",
      "charge\n",
      "cheap\n",
      "cheaper\n",
      "check\n",
      "cheers\n",
      "chi\n",
      "chicago\n",
      "child\n",
      "children\n",
      "chip\n",
      "chips\n",
      "choice\n",
      "choose\n",
      "chris\n",
      "christ\n",
      "christian\n",
      "christianity\n",
      "christians\n",
      "church\n",
      "circuit\n",
      "cities\n",
      "citizens\n",
      "city\n",
      "civil\n",
      "civilians\n",
      "ck\n",
      "claim\n",
      "claimed\n",
      "claims\n",
      "class\n",
      "clear\n",
      "clearly\n",
      "client\n",
      "clients\n",
      "clinton\n",
      "clipper\n",
      "clock\n",
      "close\n",
      "closed\n",
      "club\n",
      "code\n",
      "cold\n",
      "collection\n",
      "college\n",
      "color\n",
      "colorado\n",
      "colors\n",
      "com\n",
      "come\n",
      "comes\n",
      "coming\n",
      "command\n",
      "comment\n",
      "comments\n",
      "commercial\n",
      "committed\n",
      "committee\n",
      "common\n",
      "communications\n",
      "community\n",
      "comp\n",
      "companies\n",
      "company\n",
      "compare\n",
      "compared\n",
      "comparison\n",
      "compatible\n",
      "complete\n",
      "completely\n",
      "complex\n",
      "compression\n",
      "computer\n",
      "computers\n",
      "concept\n",
      "concerned\n",
      "conclusion\n",
      "condition\n",
      "conditions\n",
      "conference\n",
      "configuration\n",
      "conflict\n",
      "congress\n",
      "connect\n",
      "connected\n",
      "connection\n",
      "consider\n",
      "considered\n",
      "considering\n",
      "consistent\n",
      "constitution\n",
      "contact\n",
      "contain\n",
      "containing\n",
      "contains\n",
      "contest\n",
      "context\n",
      "continue\n",
      "contrib\n",
      "control\n",
      "controller\n",
      "convert\n",
      "converter\n",
      "copies\n",
      "copy\n",
      "correct\n",
      "correctly\n",
      "cost\n",
      "costs\n",
      "couldn\n",
      "council\n",
      "count\n",
      "countries\n",
      "country\n",
      "couple\n",
      "course\n",
      "court\n",
      "cover\n",
      "coverage\n",
      "covered\n",
      "cpu\n",
      "create\n",
      "created\n",
      "creation\n",
      "credit\n",
      "crime\n",
      "criminal\n",
      "criminals\n",
      "cross\n",
      "crypto\n",
      "cryptography\n",
      "cs\n",
      "culture\n",
      "cup\n",
      "current\n",
      "currently\n",
      "cut\n",
      "cx\n",
      "d9\n",
      "damage\n",
      "dangerous\n",
      "data\n",
      "database\n",
      "date\n",
      "dave\n",
      "david\n",
      "day\n",
      "days\n",
      "db\n",
      "dc\n",
      "dead\n",
      "deal\n",
      "dealer\n",
      "death\n",
      "dec\n",
      "decent\n",
      "decide\n",
      "decided\n",
      "decision\n",
      "default\n",
      "defense\n",
      "define\n",
      "defined\n",
      "definitely\n",
      "definition\n",
      "degree\n",
      "deleted\n",
      "department\n",
      "depending\n",
      "depends\n",
      "des\n",
      "described\n",
      "description\n",
      "design\n",
      "designed\n",
      "despite\n",
      "det\n",
      "details\n",
      "determine\n",
      "detroit\n",
      "develop\n",
      "developed\n",
      "development\n",
      "device\n",
      "devices\n",
      "did\n",
      "didn\n",
      "die\n",
      "died\n",
      "difference\n",
      "differences\n",
      "different\n",
      "difficult\n",
      "digital\n",
      "direct\n",
      "direction\n",
      "directly\n",
      "director\n",
      "directory\n",
      "disagree\n",
      "discuss\n",
      "discussed\n",
      "discussion\n",
      "disease\n",
      "disk\n",
      "disks\n",
      "display\n",
      "distance\n",
      "distributed\n",
      "distribution\n",
      "division\n",
      "doctor\n",
      "document\n",
      "documentation\n",
      "documents\n",
      "dod\n",
      "does\n",
      "doesn\n",
      "dog\n",
      "doing\n",
      "dollars\n",
      "domain\n",
      "don\n",
      "door\n",
      "dos\n",
      "double\n",
      "doubt\n",
      "doug\n",
      "dr\n",
      "draft\n",
      "draw\n",
      "drive\n",
      "driver\n",
      "drivers\n",
      "drives\n",
      "driving\n",
      "drop\n",
      "drug\n",
      "drugs\n",
      "earlier\n",
      "early\n",
      "earth\n",
      "easier\n",
      "easily\n",
      "east\n",
      "easy\n",
      "eat\n",
      "economic\n",
      "ed\n",
      "edge\n",
      "edition\n",
      "editor\n",
      "edu\n",
      "education\n",
      "eff\n",
      "effect\n",
      "effective\n",
      "effects\n",
      "effort\n",
      "efforts\n",
      "electrical\n",
      "electronic\n",
      "em\n",
      "email\n",
      "encrypted\n",
      "encryption\n",
      "end\n",
      "energy\n",
      "enforcement\n",
      "engine\n",
      "engineering\n",
      "english\n",
      "enter\n",
      "entire\n",
      "entirely\n",
      "entries\n",
      "entry\n",
      "environment\n",
      "equal\n",
      "equipment\n",
      "equivalent\n",
      "eric\n",
      "error\n",
      "errors\n",
      "escrow\n",
      "especially\n",
      "essentially\n",
      "established\n",
      "et\n",
      "eternal\n",
      "europe\n",
      "european\n",
      "event\n",
      "events\n",
      "eventually\n",
      "everybody\n",
      "evidence\n",
      "evil\n",
      "ex\n",
      "exact\n",
      "exactly\n",
      "example\n",
      "examples\n",
      "excellent\n",
      "excuse\n",
      "exist\n",
      "existence\n",
      "existing\n",
      "exists\n",
      "expansion\n",
      "expect\n",
      "expected\n",
      "expensive\n",
      "experience\n",
      "explain\n",
      "export\n",
      "external\n",
      "extra\n",
      "extremely\n",
      "eye\n",
      "eyes\n",
      "face\n",
      "fact\n",
      "factor\n",
      "facts\n",
      "failed\n",
      "fair\n",
      "fairly\n",
      "faith\n",
      "fall\n",
      "false\n",
      "family\n",
      "fan\n",
      "fans\n",
      "faq\n",
      "far\n",
      "fast\n",
      "faster\n",
      "father\n",
      "fault\n",
      "fax\n",
      "fbi\n",
      "fear\n",
      "feature\n",
      "features\n",
      "february\n",
      "federal\n",
      "feel\n",
      "feeling\n",
      "feet\n",
      "felt\n",
      "fi\n",
      "field\n",
      "fields\n",
      "fight\n",
      "fighting\n",
      "figure\n",
      "file\n",
      "filename\n",
      "files\n",
      "final\n",
      "finally\n",
      "finding\n",
      "fine\n",
      "firearm\n",
      "firearms\n",
      "fit\n",
      "fix\n",
      "fixed\n",
      "flame\n",
      "flames\n",
      "flight\n",
      "floor\n",
      "floppy\n",
      "flyers\n",
      "folks\n",
      "follow\n",
      "followed\n",
      "following\n",
      "follows\n",
      "font\n",
      "fonts\n",
      "food\n",
      "force\n",
      "forces\n",
      "foreign\n",
      "forget\n",
      "form\n",
      "format\n",
      "formats\n",
      "forward\n",
      "foundation\n",
      "frame\n",
      "francisco\n",
      "frank\n",
      "free\n",
      "freedom\n",
      "french\n",
      "frequently\n",
      "friend\n",
      "friends\n",
      "ftp\n",
      "fuel\n",
      "fully\n",
      "fun\n",
      "function\n",
      "functions\n",
      "funds\n",
      "future\n",
      "g9\n",
      "g9v\n",
      "gain\n",
      "game\n",
      "games\n",
      "gas\n",
      "gave\n",
      "gay\n",
      "general\n",
      "generally\n",
      "generation\n",
      "genocide\n",
      "george\n",
      "german\n",
      "germany\n",
      "gets\n",
      "getting\n",
      "gif\n",
      "given\n",
      "gives\n",
      "giving\n",
      "giz\n",
      "gk\n",
      "gm\n",
      "goal\n",
      "goals\n",
      "god\n",
      "goes\n",
      "going\n",
      "gone\n",
      "good\n",
      "gordon\n",
      "got\n",
      "gotten\n",
      "gov\n",
      "government\n",
      "grant\n",
      "graphics\n",
      "great\n",
      "greater\n",
      "greatly\n",
      "greek\n",
      "green\n",
      "ground\n",
      "group\n",
      "groups\n",
      "guess\n",
      "guide\n",
      "gun\n",
      "guns\n",
      "guy\n",
      "guys\n",
      "half\n",
      "hall\n",
      "hand\n",
      "handle\n",
      "hands\n",
      "happen\n",
      "happened\n",
      "happening\n",
      "happens\n",
      "happy\n",
      "hard\n",
      "hardware\n",
      "hasn\n",
      "hate\n",
      "haven\n",
      "having\n",
      "hd\n",
      "head\n",
      "heads\n",
      "health\n",
      "hear\n",
      "heard\n",
      "heart\n",
      "heat\n",
      "heaven\n",
      "heavy\n",
      "held\n",
      "hell\n",
      "hello\n",
      "help\n",
      "helpful\n",
      "helps\n",
      "hey\n",
      "hi\n",
      "high\n",
      "higher\n",
      "highly\n",
      "historical\n",
      "history\n",
      "hit\n",
      "hockey\n",
      "hold\n",
      "hole\n",
      "holy\n",
      "home\n",
      "hope\n",
      "hospital\n",
      "host\n",
      "hot\n",
      "hour\n",
      "hours\n",
      "house\n",
      "hp\n",
      "huge\n",
      "human\n",
      "humans\n",
      "hurt\n",
      "hz\n",
      "ibm\n",
      "ice\n",
      "id\n",
      "ide\n",
      "idea\n",
      "ideas\n",
      "ii\n",
      "illegal\n",
      "image\n",
      "images\n",
      "imagine\n",
      "immediately\n",
      "impact\n",
      "implementation\n",
      "important\n",
      "impossible\n",
      "include\n",
      "included\n",
      "includes\n",
      "including\n",
      "increase\n",
      "increased\n",
      "independent\n",
      "index\n",
      "individual\n",
      "individuals\n",
      "industry\n",
      "info\n",
      "information\n",
      "innocent\n",
      "input\n",
      "inside\n",
      "install\n",
      "installed\n",
      "instance\n",
      "instead\n",
      "institute\n",
      "insurance\n",
      "int\n",
      "intended\n",
      "interested\n",
      "interesting\n",
      "interface\n",
      "internal\n",
      "international\n",
      "internet\n",
      "interpretation\n",
      "involved\n",
      "isa\n",
      "islam\n",
      "islamic\n",
      "island\n",
      "isn\n",
      "israel\n",
      "israeli\n",
      "issue\n",
      "issues\n",
      "items\n",
      "james\n",
      "japanese\n",
      "jesus\n",
      "jewish\n",
      "jews\n",
      "jim\n",
      "job\n",
      "jobs\n",
      "joe\n",
      "john\n",
      "jose\n",
      "joseph\n",
      "journal\n",
      "jpeg\n",
      "judge\n",
      "june\n",
      "just\n",
      "justice\n",
      "k8\n",
      "keeping\n",
      "kept\n",
      "key\n",
      "keyboard\n",
      "keys\n",
      "kids\n",
      "kill\n",
      "killed\n",
      "killing\n",
      "kind\n",
      "king\n",
      "kings\n",
      "km\n",
      "kn\n",
      "knew\n",
      "know\n",
      "knowing\n",
      "knowledge\n",
      "known\n",
      "knows\n",
      "koresh\n",
      "la\n",
      "lab\n",
      "lack\n",
      "land\n",
      "language\n",
      "large\n",
      "larger\n",
      "larry\n",
      "late\n",
      "later\n",
      "latest\n",
      "launch\n",
      "law\n",
      "laws\n",
      "lc\n",
      "lcs\n",
      "lead\n",
      "leaders\n",
      "leading\n",
      "leads\n",
      "leafs\n",
      "league\n",
      "learn\n",
      "leave\n",
      "led\n",
      "left\n",
      "legal\n",
      "legitimate\n",
      "length\n",
      "let\n",
      "letter\n",
      "level\n",
      "levels\n",
      "lg\n",
      "lib\n",
      "library\n",
      "license\n",
      "lie\n",
      "life\n",
      "light\n",
      "like\n",
      "likely\n",
      "limit\n",
      "limited\n",
      "line\n",
      "lines\n",
      "list\n",
      "listed\n",
      "listen\n",
      "lists\n",
      "little\n",
      "live\n",
      "lived\n",
      "lives\n",
      "living\n",
      "lj\n",
      "lk\n",
      "ll\n",
      "load\n",
      "local\n",
      "location\n",
      "lock\n",
      "logic\n",
      "long\n",
      "longer\n",
      "look\n",
      "looked\n",
      "looking\n",
      "looks\n",
      "lord\n",
      "los\n",
      "lose\n",
      "loss\n",
      "lost\n",
      "lot\n",
      "lots\n",
      "louis\n",
      "love\n",
      "low\n",
      "lower\n",
      "luck\n",
      "lunar\n",
      "m0\n",
      "m3\n",
      "m5\n",
      "m_\n",
      "ma\n",
      "mac\n",
      "machine\n",
      "machines\n",
      "macintosh\n",
      "magazine\n",
      "mail\n",
      "mailing\n",
      "main\n",
      "major\n",
      "majority\n",
      "make\n",
      "makes\n",
      "making\n",
      "man\n",
      "management\n",
      "manager\n",
      "manner\n",
      "manual\n",
      "manufacturers\n",
      "map\n",
      "march\n",
      "mark\n",
      "market\n",
      "mars\n",
      "mary\n",
      "mass\n",
      "master\n",
      "material\n",
      "math\n",
      "matter\n",
      "matthew\n",
      "max\n",
      "maybe\n",
      "mb\n",
      "mc\n",
      "md\n",
      "mean\n",
      "meaning\n",
      "means\n",
      "meant\n",
      "media\n",
      "medical\n",
      "medicine\n",
      "meet\n",
      "meeting\n",
      "meg\n",
      "member\n",
      "members\n",
      "memory\n",
      "men\n",
      "mention\n",
      "mentioned\n",
      "merely\n",
      "message\n",
      "messages\n",
      "met\n",
      "method\n",
      "methods\n",
      "mhz\n",
      "mi\n",
      "michael\n",
      "microsoft\n",
      "middle\n",
      "mike\n",
      "mil\n",
      "miles\n",
      "military\n",
      "militia\n",
      "million\n",
      "min\n",
      "mind\n",
      "minnesota\n",
      "minor\n",
      "minority\n",
      "minute\n",
      "minutes\n",
      "misc\n",
      "missed\n",
      "missing\n",
      "mission\n",
      "mistake\n",
      "mit\n",
      "mk\n",
      "ml\n",
      "mm\n",
      "mode\n",
      "model\n",
      "models\n",
      "modem\n",
      "modern\n",
      "modes\n",
      "modified\n",
      "moment\n",
      "mon\n",
      "money\n",
      "monitor\n",
      "month\n",
      "months\n",
      "montreal\n",
      "moon\n",
      "moral\n",
      "morality\n",
      "morning\n",
      "mother\n",
      "motherboard\n",
      "motif\n",
      "motorcycle\n",
      "mouse\n",
      "moved\n",
      "movement\n",
      "moving\n",
      "mp\n",
      "mq\n",
      "mr\n",
      "ms\n",
      "msg\n",
      "mt\n",
      "mu\n",
      "multi\n",
      "multiple\n",
      "murder\n",
      "muslim\n",
      "muslims\n",
      "mv\n",
      "mw\n",
      "myers\n",
      "named\n",
      "names\n",
      "nasa\n",
      "nation\n",
      "national\n",
      "nations\n",
      "natural\n",
      "nature\n",
      "navy\n",
      "nazi\n",
      "nazis\n",
      "near\n",
      "nearly\n",
      "nec\n",
      "necessarily\n",
      "necessary\n",
      "need\n",
      "needed\n",
      "needs\n",
      "net\n",
      "network\n",
      "neutral\n",
      "new\n",
      "news\n",
      "newsgroup\n",
      "newsgroups\n",
      "nhl\n",
      "nice\n",
      "night\n",
      "nj\n",
      "nl\n",
      "noise\n",
      "non\n",
      "normal\n",
      "normally\n",
      "north\n",
      "note\n",
      "noted\n",
      "notes\n",
      "notice\n",
      "noticed\n",
      "nrhj\n",
      "nsa\n",
      "nt\n",
      "nuclear\n",
      "null\n",
      "number\n",
      "numbers\n",
      "ny\n",
      "object\n",
      "objective\n",
      "objects\n",
      "obtain\n",
      "obtained\n",
      "obvious\n",
      "obviously\n",
      "offer\n",
      "offers\n",
      "office\n",
      "official\n",
      "officials\n",
      "oh\n",
      "oil\n",
      "ok\n",
      "okay\n",
      "old\n",
      "older\n",
      "ones\n",
      "open\n",
      "operating\n",
      "operation\n",
      "operations\n",
      "opinion\n",
      "opinions\n",
      "opposed\n",
      "option\n",
      "options\n",
      "orbit\n",
      "order\n",
      "org\n",
      "organization\n",
      "organizations\n",
      "original\n",
      "os\n",
      "output\n",
      "outside\n",
      "owner\n",
      "owners\n",
      "p2\n",
      "package\n",
      "packages\n",
      "page\n",
      "pages\n",
      "paid\n",
      "pain\n",
      "panel\n",
      "paper\n",
      "papers\n",
      "parallel\n",
      "parents\n",
      "park\n",
      "particular\n",
      "particularly\n",
      "parties\n",
      "parts\n",
      "party\n",
      "pass\n",
      "passed\n",
      "past\n",
      "path\n",
      "patients\n",
      "paul\n",
      "pay\n",
      "pc\n",
      "peace\n",
      "penalty\n",
      "people\n",
      "percent\n",
      "perfect\n",
      "performance\n",
      "period\n",
      "person\n",
      "personal\n",
      "personally\n",
      "peter\n",
      "pgp\n",
      "philadelphia\n",
      "phone\n",
      "physical\n",
      "pick\n",
      "picture\n",
      "pictures\n",
      "piece\n",
      "pin\n",
      "pit\n",
      "pitt\n",
      "pittsburgh\n",
      "pl\n",
      "place\n",
      "places\n",
      "plan\n",
      "plane\n",
      "play\n",
      "played\n",
      "player\n",
      "players\n",
      "playing\n",
      "plus\n",
      "pm\n",
      "point\n",
      "pointed\n",
      "points\n",
      "police\n",
      "policy\n",
      "political\n",
      "politics\n",
      "poor\n",
      "pope\n",
      "popular\n",
      "population\n",
      "port\n",
      "portable\n",
      "ports\n",
      "position\n",
      "positive\n",
      "possibility\n",
      "possible\n",
      "possibly\n",
      "post\n",
      "posted\n",
      "poster\n",
      "posting\n",
      "postings\n",
      "posts\n",
      "postscript\n",
      "potential\n",
      "power\n",
      "pp\n",
      "practice\n",
      "pre\n",
      "prefer\n",
      "present\n",
      "presented\n",
      "president\n",
      "press\n",
      "pressure\n",
      "pretty\n",
      "prevent\n",
      "previous\n",
      "price\n",
      "prices\n",
      "print\n",
      "printer\n",
      "printf\n",
      "prior\n",
      "privacy\n",
      "private\n",
      "pro\n",
      "probably\n",
      "problem\n",
      "problems\n",
      "process\n",
      "processing\n",
      "processor\n",
      "produce\n",
      "produced\n",
      "product\n",
      "products\n",
      "professor\n",
      "program\n",
      "programming\n",
      "programs\n",
      "project\n",
      "proof\n",
      "proper\n",
      "properly\n",
      "property\n",
      "proposal\n",
      "proposed\n",
      "protect\n",
      "protection\n",
      "prove\n",
      "provide\n",
      "provided\n",
      "provides\n",
      "providing\n",
      "ps\n",
      "pt\n",
      "pts\n",
      "pub\n",
      "public\n",
      "published\n",
      "puck\n",
      "pull\n",
      "purchase\n",
      "purpose\n",
      "purposes\n",
      "push\n",
      "putting\n",
      "qax\n",
      "ql\n",
      "qq\n",
      "quality\n",
      "question\n",
      "questions\n",
      "quick\n",
      "quickly\n",
      "quite\n",
      "quote\n",
      "quoted\n",
      "r4\n",
      "r5\n",
      "ra\n",
      "race\n",
      "radar\n",
      "radio\n",
      "ram\n",
      "ran\n",
      "random\n",
      "range\n",
      "rangers\n",
      "rate\n",
      "rates\n",
      "ray\n",
      "reach\n",
      "read\n",
      "readers\n",
      "reading\n",
      "ready\n",
      "real\n",
      "reality\n",
      "realize\n",
      "really\n",
      "reason\n",
      "reasonable\n",
      "reasons\n",
      "rec\n",
      "recall\n",
      "receive\n",
      "received\n",
      "recent\n",
      "recently\n",
      "recognize\n",
      "recommend\n",
      "record\n",
      "red\n",
      "reduce\n",
      "refer\n",
      "reference\n",
      "references\n",
      "regarding\n",
      "regards\n",
      "registration\n",
      "regular\n",
      "related\n",
      "relationship\n",
      "relatively\n",
      "release\n",
      "released\n",
      "relevant\n",
      "religion\n",
      "religions\n",
      "religious\n",
      "remain\n",
      "remember\n",
      "remote\n",
      "remove\n",
      "replace\n",
      "replaced\n",
      "replies\n",
      "reply\n",
      "report\n",
      "reported\n",
      "reports\n",
      "request\n",
      "requests\n",
      "require\n",
      "required\n",
      "requirements\n",
      "requires\n",
      "research\n",
      "resolution\n",
      "resource\n",
      "resources\n",
      "respect\n",
      "respond\n",
      "response\n",
      "responses\n",
      "responsibility\n",
      "responsible\n",
      "rest\n",
      "result\n",
      "results\n",
      "return\n",
      "returned\n",
      "review\n",
      "richard\n",
      "ride\n",
      "riding\n",
      "right\n",
      "rights\n",
      "ripem\n",
      "risk\n",
      "road\n",
      "robert\n",
      "rocket\n",
      "role\n",
      "rom\n",
      "ron\n",
      "room\n",
      "root\n",
      "round\n",
      "routines\n",
      "rsa\n",
      "rule\n",
      "rules\n",
      "run\n",
      "running\n",
      "runs\n",
      "russia\n",
      "russian\n",
      "s1\n",
      "safe\n",
      "safety\n",
      "said\n",
      "sale\n",
      "sales\n",
      "sample\n",
      "san\n",
      "satellite\n",
      "save\n",
      "saved\n",
      "saw\n",
      "say\n",
      "saying\n",
      "says\n",
      "scale\n",
      "scheme\n",
      "school\n",
      "sci\n",
      "science\n",
      "scientific\n",
      "scoring\n",
      "scott\n",
      "screen\n",
      "scripture\n",
      "scsi\n",
      "se\n",
      "sea\n",
      "search\n",
      "season\n",
      "second\n",
      "seconds\n",
      "secret\n",
      "secretary\n",
      "section\n",
      "secure\n",
      "security\n",
      "seeing\n",
      "seen\n",
      "select\n",
      "self\n",
      "sell\n",
      "selling\n",
      "semi\n",
      "senate\n",
      "send\n",
      "sending\n",
      "sense\n",
      "sent\n",
      "separate\n",
      "serial\n",
      "series\n",
      "seriously\n",
      "server\n",
      "servers\n",
      "service\n",
      "services\n",
      "session\n",
      "set\n",
      "sets\n",
      "setting\n",
      "setup\n",
      "seven\n",
      "sex\n",
      "sgi\n",
      "shall\n",
      "share\n",
      "shared\n",
      "shell\n",
      "shipping\n",
      "short\n",
      "shot\n",
      "shots\n",
      "shouldn\n",
      "showed\n",
      "shown\n",
      "shows\n",
      "shuttle\n",
      "si\n",
      "sign\n",
      "signal\n",
      "significant\n",
      "similar\n",
      "simms\n",
      "simple\n",
      "simply\n",
      "sin\n",
      "single\n",
      "site\n",
      "sites\n",
      "situation\n",
      "size\n",
      "sk\n",
      "sl\n",
      "slave\n",
      "slightly\n",
      "slot\n",
      "slow\n",
      "small\n",
      "smaller\n",
      "smith\n",
      "social\n",
      "society\n",
      "software\n",
      "solar\n",
      "sold\n",
      "soldiers\n",
      "solid\n",
      "solution\n",
      "somebody\n",
      "somewhat\n",
      "son\n",
      "soon\n",
      "sorry\n",
      "sort\n",
      "sound\n",
      "sounds\n",
      "source\n",
      "sources\n",
      "south\n",
      "soviet\n",
      "sp\n",
      "space\n",
      "spacecraft\n",
      "speak\n",
      "speaking\n",
      "special\n",
      "specific\n",
      "specifically\n",
      "speech\n",
      "speed\n",
      "spend\n",
      "spent\n",
      "spirit\n",
      "spot\n",
      "sq\n",
      "st\n",
      "stage\n",
      "stand\n",
      "standard\n",
      "standards\n",
      "star\n",
      "stars\n",
      "start\n",
      "started\n",
      "starting\n",
      "starts\n",
      "state\n",
      "stated\n",
      "statement\n",
      "statements\n",
      "states\n",
      "station\n",
      "stats\n",
      "status\n",
      "stay\n",
      "step\n",
      "steve\n",
      "stick\n",
      "stop\n",
      "stopped\n",
      "store\n",
      "story\n",
      "straight\n",
      "stream\n",
      "street\n",
      "string\n",
      "strong\n",
      "structure\n",
      "student\n",
      "studies\n",
      "study\n",
      "stuff\n",
      "stupid\n",
      "style\n",
      "subject\n",
      "success\n",
      "sufficient\n",
      "suggest\n",
      "suggested\n",
      "suggestions\n",
      "sumgait\n",
      "summary\n",
      "summer\n",
      "sun\n",
      "sunday\n",
      "super\n",
      "supply\n",
      "support\n",
      "supported\n",
      "supports\n",
      "suppose\n",
      "supposed\n",
      "sure\n",
      "surely\n",
      "surface\n",
      "suspect\n",
      "switch\n",
      "sys\n",
      "systems\n",
      "t5\n",
      "table\n",
      "taken\n",
      "takes\n",
      "taking\n",
      "talk\n",
      "talking\n",
      "tank\n",
      "tape\n",
      "tar\n",
      "tax\n",
      "te\n",
      "team\n",
      "teams\n",
      "technical\n",
      "technology\n",
      "telephone\n",
      "tell\n",
      "telling\n",
      "tells\n",
      "tend\n",
      "term\n",
      "terms\n",
      "test\n",
      "tests\n",
      "texas\n",
      "text\n",
      "thank\n",
      "thanks\n",
      "theory\n",
      "thing\n",
      "things\n",
      "think\n",
      "thinking\n",
      "thomas\n",
      "thought\n",
      "thousands\n",
      "thread\n",
      "threat\n",
      "throw\n",
      "time\n",
      "times\n",
      "title\n",
      "tm\n",
      "today\n",
      "told\n",
      "tom\n",
      "took\n",
      "tool\n",
      "toolkit\n",
      "tools\n",
      "topic\n",
      "tor\n",
      "toronto\n",
      "total\n",
      "totally\n",
      "town\n",
      "tq\n",
      "track\n",
      "trade\n",
      "traffic\n",
      "training\n",
      "transfer\n",
      "translation\n",
      "treatment\n",
      "trial\n",
      "tried\n",
      "troops\n",
      "trouble\n",
      "true\n",
      "truly\n",
      "trust\n",
      "truth\n",
      "try\n",
      "trying\n",
      "turbo\n",
      "turkey\n",
      "turkish\n",
      "turks\n",
      "turn\n",
      "turned\n",
      "tv\n",
      "twice\n",
      "type\n",
      "types\n",
      "typical\n",
      "uk\n",
      "understand\n",
      "understanding\n",
      "unfortunately\n",
      "unit\n",
      "united\n",
      "universe\n",
      "university\n",
      "unix\n",
      "unless\n",
      "upgrade\n",
      "usa\n",
      "use\n",
      "used\n",
      "useful\n",
      "usenet\n",
      "user\n",
      "users\n",
      "uses\n",
      "using\n",
      "usr\n",
      "usual\n",
      "usually\n",
      "uunet\n",
      "uw\n",
      "valid\n",
      "value\n",
      "values\n",
      "van\n",
      "vancouver\n",
      "variety\n",
      "various\n",
      "ve\n",
      "vehicle\n",
      "version\n",
      "versions\n",
      "vga\n",
      "video\n",
      "view\n",
      "views\n",
      "villages\n",
      "virtual\n",
      "visual\n",
      "voice\n",
      "volume\n",
      "vote\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs\n",
      "wait\n",
      "waiting\n",
      "wall\n",
      "want\n",
      "wanted\n",
      "wants\n",
      "war\n",
      "warning\n",
      "washington\n",
      "wasn\n",
      "waste\n",
      "watch\n",
      "water\n",
      "way\n",
      "ways\n",
      "weak\n",
      "weapon\n",
      "weapons\n",
      "week\n",
      "weeks\n",
      "weight\n",
      "welcome\n",
      "went\n",
      "west\n",
      "western\n",
      "white\n",
      "wide\n",
      "widget\n",
      "widgets\n",
      "wife\n",
      "willing\n",
      "win\n",
      "window\n",
      "windows\n",
      "wings\n",
      "winning\n",
      "wire\n",
      "wiring\n",
      "wish\n",
      "wm\n",
      "woman\n",
      "women\n",
      "won\n",
      "wonder\n",
      "wondering\n",
      "word\n",
      "words\n",
      "work\n",
      "worked\n",
      "working\n",
      "works\n",
      "world\n",
      "worry\n",
      "worse\n",
      "worth\n",
      "wouldn\n",
      "write\n",
      "writes\n",
      "writing\n",
      "written\n",
      "wrong\n",
      "wrote\n",
      "wt\n",
      "ww\n",
      "x11\n",
      "x11r5\n",
      "xlib\n",
      "xt\n",
      "xterm\n",
      "yeah\n",
      "year\n",
      "years\n",
      "yes\n",
      "yesterday\n",
      "york\n",
      "young\n",
      "zip\n",
      "zone\n"
     ]
    }
   ],
   "source": [
    "for word in idx_to_word:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "- Even if there are millions of documents $\\mathbf{a}_i\\in\\mathbb{R}^{d}$ there is likely only a small number of topics $\\mathbf{w}_1,\\ldots,\\mathbf{w}_r\\in\\mathbb{R}^{d}$.\n",
    "- **Hypothesis** each document is (approximately) a linear combination of topics: $\\mathbf{a}_i = \\sum_{\\ell=1}^{r}\\alpha_{\\ell}\\mathbf{w}_{\\ell}$.\n",
    "- Important that $\\mathbf{w}_{\\ell}\\in\\mathbb{R}^{d}$ are non-negative. Why?\n",
    "\n",
    "![alt text](Topic_Modelling.png) \n",
    " - Image from [here](https://towardsdatascience.com/topic-modeling-quora-questions-with-lda-nmf-aff8dce5e1dd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization\n",
    "\n",
    "- Suppose that $A\\in\\mathbb{R}^{d\\times n}$. \n",
    "- Let $\\mathbb{R}^{d\\times r}_{+}$ denote all $d\\times r$ matrices with non-negative entries.\n",
    "- Similarly for $\\mathbb{R}^{r\\times n}_{+}$\n",
    "- Want $W\\in \\mathbb{R}^{d\\times r}_{+}$ and $H \\in \\mathbb{R}^{r\\times n}_{+}$ such that $\\|A - WH\\|_{F}$ is as small as possible.\n",
    "- **NMF:** $ W,H = \\text{argmin}_{U\\in \\mathbb{R}^{d\\times r}_{+}, V\\in\\mathbb{R}^{r\\times n}_{+}}\\|A - UV\\|^{2}_{F}$\n",
    "\n",
    "*****\n",
    "### Properties of NMF\n",
    "\n",
    " - Cannot expect $A = WH$ (like for SVD). Usually write $A\\approx WH$.\n",
    " - NMF problem does not have a unique solution (like best rank $k$ approx. problem does). \n",
    "*****\n",
    "\n",
    "### Algorithms for NMF\n",
    " - Suppose we know $W$. Then finding $H$ becomes (non-negative) matrix least squares:\n",
    " \n",
    " $ H = \\text{argmin}_{V\\in\\mathbb{R}^{r\\times n}_{+}}\\|A - WV\\|^{2}_{F}$\n",
    " \n",
    " - What if we knew $H$? Then finding $W$ is also (non-negative) matrix least squares:\n",
    " \n",
    " $W = \\text{argmin}_{U\\in\\mathbb{R}^{d\\times r}_{+}}\\|A - UH\\|^{2}_{F}$ \n",
    " \n",
    " - **Idea:** If we have approximations $W_{k},H_{k}$:\n",
    "     + Use $W_{k}$ to find new approx. of $H$:\n",
    "         $ H_{k+1} = \\text{argmin}_{V\\in\\mathbb{R}^{r\\times n}_{+}}\\|A - W_{k}V\\|^{2}_{F} = \\text{argmin}_{\\mathbf{v}_{i} \\geq 0 \\ \\text{ for } j=1,\\ldots n} \\sum_{j=1}^{n}\\|\\mathbf{a}_{j} - W_{k}\\mathbf{v}_j\\|_{2}^{2}$\n",
    "     + Note that $V = \\left[\\begin{matrix} \\mathbf{v}_1 & \\ldots & \\mathbf{v}_{r} \\end{matrix}\\right]$\n",
    "     + $\\mathbf{v}_{i} = \\text{nnls}(W_{k},\\mathbf{a}_i)$ for $i=1,\\ldots, n$\n",
    "     + $\\|A - W_{k}V\\|^{2}_{F} = \\sum_{i=1}^{n}\\sum_{j=1}^{d}\\left(A_{ij} - \\left(W_{k}V\\right)_{ij}\\right)^{2}$\n",
    "     + Can do some rewriting: $A_{ij} = \\mathbf{a}_{i,j}$\n",
    "     + Then, use $H_{k+1}$ to find a new approx. of $W$:\n",
    "         $ W_{k+1} = \\text{argmin}_{U\\in\\mathbb{R}^{d\\times r}_{+}}\\|A - UH_{k+1}\\|^{2}_{F}$\n",
    "     + Extra steps: Know that $\\|A - UH_{k+1}\\|^{2}_{F} = \\|\\left(A - UH_{k+1}\\right)^{\\top}\\|^{2}_{F} = \\|A^{\\top} - H_{k+1}^{\\top}U^{\\top}\\|^{2}_{F}$\n",
    "     + $\\mathbf{u}_i = \\text{nnls}(H_{k+1}^{\\top},\\mathbf{r}_i)$ where $\\mathbf{r}_i$ is the $i$-th row of $A$\n",
    "     + Repeat.\n",
    "\n",
    "- This is **Alternating (Non-negative) Least Squares** (ALS).\n",
    "- *Will implement this algorithm when we discuss projected gradient descent*\n",
    "- See also *Algorithms for non-negative matrix factorization* by Lee and Heung (NIPS 2001) for a faster algorithm. \n",
    "\n",
    "**********\n",
    "\n",
    "### NMF for Topic Modeling\n",
    "\n",
    "- Suppose that $A\\in\\mathbb{R}^{d\\times n}$ with *columns* corresponding to documents/ data points.\n",
    "- Assume we have found $A \\approx WH$ with $W\\in \\mathbb{R}^{d\\times r}_{+}$ and $H \\in \\mathbb{R}^{r\\times n}_{+}$.\n",
    "- *Columns* of $W$ represent topics.\n",
    "- For each column $\\mathbf{a}_j$ can write $\\mathbf{a}_{j} \\approx \\sum_{i=1}^{r} H_{ij}\\mathbf{w}_i$. \n",
    "- $H_{ij} = $ correlation of document $j$ with topic $i$.\n",
    "- Choosing $r$ is a bit of an art.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: really,say,way,said,right,did,ve,good,time,people\n",
      "Topic 2: appreciated,hi,information,program,need,software,looking,help,use,mail\n",
      "Topic 3: lord,church,christians,christian,believe,faith,christ,bible,jesus,god\n",
      "Topic 4: algorithm,public,use,escrow,government,keys,clipper,encryption,chip,key\n",
      "Topic 5: problem,cd,floppy,controller,ide,hard,disk,drives,scsi,drive\n",
      "Topic 6: 15,20,price,condition,offer,shipping,10,new,sale,00\n",
      "Topic 7: directory,version,problem,ms,program,running,files,dos,file,windows\n",
      "Topic 8: player,win,hockey,play,season,players,year,games,team,game\n",
      "Topic 9: colorado,pub,cc,university,cs,soon,banks,gordon,pitt,edu\n",
      "Topic 10: new,oil,speed,dealer,miles,good,engine,bike,cars,car\n",
      "Topic 11: ram,color,driver,vga,bus,cards,drivers,monitor,video,card\n",
      "Topic 12: exactly,exist,say,work,new,doesn,mean,anybody,know,does\n",
      "Topic 13: sorry,anybody,good,need,let,want,think,people,know,don\n",
      "Topic 14: appreciate,wondering,got,info,tell,ve,hi,mail,advance,thanks\n",
      "Topic 15: new,wanted,mean,thought,know,oh,ll,wondering,don,just\n",
      "Topic 16: things,sell,new,make,lot,sound,look,looks,sounds,like\n",
      "Topic 17: address,uunet,jim,sun,bob,internet,dave,article,list,com\n",
      "Topic 18: use,using,problem,widget,server,display,motif,manager,application,window\n",
      "Topic 19: nasa,agree,makes,say,lot,wrong,does,science,space,think\n",
      "Topic 20: attacks,land,war,peace,arabs,jewish,arab,jews,israeli,israel\n"
     ]
    }
   ],
   "source": [
    "# apply NMF\n",
    "nmf = NMF(n_components=20, solver=\"mu\")\n",
    "H = nmf.fit_transform(A)\n",
    "W = nmf.components_\n",
    " \n",
    "# print the topics\n",
    " \n",
    "for i, topic in enumerate(W):\n",
    " \n",
    "    print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in idx_to_word[topic.argsort()[-10:]]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Retrieval\n",
    "- **Problem:**\n",
    "    - Given corpus $\\mathcal{D} = \\{d_1,\\ldots, d_n\\}$.\n",
    "    - Suppose a *new document* $q_1$ arrives.  \n",
    "    - **Goal** return $a_{i}$ which is the best semantic match for $q_1$\n",
    "\n",
    "- Typically, vectorize $\\mathcal{D}$ to $A$ **offline**.\n",
    "- **In practice** want to return match quickly ($\\sim 0.1 $ seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1\n",
    " - Recall documents have been **vectorized** to $\\mathbf{a}_1,\\ldots,\\mathbf{a}_n\\in\\mathbb{R}^{d}$ \n",
    " - Vectorize $q_1$ in the same manner: $q_1 \\to \\mathbf{y}_1\\in\\mathbb{R}^{d}$\n",
    " - Return article which is **closest** to $\\mathbf{y}_1$.\n",
    " - We can be flexible with how we measure closest:\n",
    "     - $\\ell_2$ or Euclidean distance: $\\|\\mathbf{y}_1 - \\mathbf{a}_i\\|$.\n",
    "     - Cosine distance: $\\frac{\\mathbf{y}^{\\top}\\mathbf{a}_i}{\\|\\mathbf{y}\\|_2\\|\\mathbf{a}_i\\|_2}$ \n",
    " - **Question:** When might cosine distance be preferable? \n",
    " \n",
    " *****\n",
    " \n",
    " ### Approach 2\n",
    "- Use the **Semantic Structure**\n",
    "- Suppose we have nmf: $A \\approx WH$ for NMF) \n",
    "- As established earlier, this means that $\\mathbf{a}_{j} \\approx \\sum_{i=1}^{r}H_{ij}\\mathbf{w}_i$\n",
    "- Let $\\mathbf{h}_{j}$ denote $j$-th *column* of $H$. Then $\\sum_{i=1}^{r}H_{ij}\\mathbf{w}_i = h_{j,1}\\mathbf{w}_{1} + h_{j,2}\\mathbf{w}_{2} + \\ldots + h_{j,r}\\mathbf{w}_{r}$. \n",
    "- **Low dimensional embedding:** $\\mathbf{a}_{j} \\mapsto \\mathbf{h}_{j}\\in\\mathbb{R}^{r}$\n",
    "- Can be more informative to measure distance in *topic space* $\\mathbb{R}^{r}$. \n",
    "- **Idea** Project $\\mathbf{y}_1$ to the *topic space* first, then measure distance.\n",
    "\n",
    "****\n",
    "\n",
    "Recommended further reading: [Word Mover's Distance](http://proceedings.mlr.press/v37/kusnerb15.pdf?source=post_page---------------------------)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "v1 = np.random.randn(4)\n",
    "v2 = np.random.randn(4)\n",
    "v3 = np.random.randn(4)\n",
    "v4 = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scipy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6b1d6e1e4038>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mV_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mV_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scipy' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
